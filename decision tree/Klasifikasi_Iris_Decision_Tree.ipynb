{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade numpy\n",
        "# !pip install --upgrade pandas\n",
        "# !pip install --upgrade scikit-learn\n",
        "# import sklearn\n",
        "# print(sklearn.__version__)"
      ],
      "metadata": {
        "id": "3n36hU6nQjMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Persiapan dan Visualisasi Dataset**"
      ],
      "metadata": {
        "id": "NjZI4dd9GxL7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y2VHFL9Ft_G"
      },
      "outputs": [],
      "source": [
        "# import library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "\n",
        "# load dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# visualisasi dataset\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
        "df['Target'] = pd.Series(iris.target)\n",
        "\n",
        "print(\"Data Head: \\n\", df.head(), \"\\n\")\n",
        "print(\"Data Distribution: \\n\", df.describe(), \"\\n\")\n",
        "sns.pairplot(df, hue='Target', palette=(\"tab10\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Splitting, Standarisasi, dan Normalisasi Dataset**"
      ],
      "metadata": {
        "id": "KjpngnIwG3Ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting dataset into train-set and test-set\n",
        "from sklearn.model_selection import train_test_split\n",
        "x = iris.data # feature\n",
        "y = iris.target # label (3 kategori: 0, 1, 2)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "print(\"x_train: \", x_train.shape)\n",
        "print(\"y_test: \", y_test.shape)\n",
        "print(\"x_test: \", x_test.shape)\n",
        "print(\"y_train: \", y_train.shape)\n",
        "\n",
        "print(\"\\nContoh data testing: \\n\", x_test)"
      ],
      "metadata": {
        "id": "O9Yj55sLHAGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standarisasi dataset (distribusi diratakan menjadi sekitar 0 dan ada negatifnya)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "print(\"Data setelah standarisasi: \\n\", x_test)"
      ],
      "metadata": {
        "id": "mfzOssd2HGMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalisasi dataset (menormalisasi skala menjadi -1x1)\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer = Normalizer().fit(x_train)\n",
        "x_train = normalizer.transform(x_train)\n",
        "x_test = normalizer.transform(x_test)\n",
        "\n",
        "print(\"Data setelah normalisasi: \\n\", x_test)"
      ],
      "metadata": {
        "id": "xOVqilxlHlhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Model**"
      ],
      "metadata": {
        "id": "XUHDbHZnIQr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# untuned model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "NN3bdKhXIU2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tuned model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(criterion='entropy', max_depth=30, min_samples_split=3)\n",
        "dt.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "9AZZr_OvR4Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Validasi Model**"
      ],
      "metadata": {
        "id": "ZjqATwPbOE7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = dt.predict(x_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Akurasi: \", round(accuracy_score(y_test, y_pred) * 100, 2))\n",
        "print(\"Presisi: \", round(precision_score(y_test, y_pred, average='weighted') * 100, 2))\n",
        "print(\"Recall : \", round(recall_score(y_test, y_pred, average='weighted') * 100, 2), \"\\n\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "id": "O_R-9SUxOKIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualisasi Hasil**"
      ],
      "metadata": {
        "id": "i02RbErgOT2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from io import StringIO ## for Python 3\n",
        "from IPython.display import Image\n",
        "import pydotplus\n",
        "\n",
        "dot_data = StringIO()\n",
        "export_graphviz(dt, out_file = dot_data, filled = True, rounded = True, special_characters = True,\n",
        "    feature_names = iris.feature_names, class_names=['0','1', '2'])\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "graph.write_png('iris.png')\n",
        "Image(graph.create_png())"
      ],
      "metadata": {
        "id": "-iL-uF8LOWvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series containing feature importances from the model and feature names from the training data\n",
        "feature_importances = pd.Series(dt.feature_importances_, index=iris.feature_names).sort_values(ascending=False)\n",
        "\n",
        "# Plot a simple bar chart\n",
        "feature_importances.plot.bar();"
      ],
      "metadata": {
        "id": "5jgDqFq4TzdA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}